UNDERSTANDING ADJOINT-BASED OPTIMIZATION IN FIREDRAKE
======================================================

This document explains what blocks, tapes, and adjoint methods are, and why 
they're so much more efficient than finite differences.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 1: THE FUNDAMENTAL PROBLEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You want to find parameters Î¸ that minimize a loss function J:

    minimize J(Î¸) = ||simulation(Î¸) - observations||Â²

To optimize, you need:
  1. The loss value J(Î¸)
  2. The gradient âˆ‡J(Î¸) = [âˆ‚J/âˆ‚Î¸â‚, âˆ‚J/âˆ‚Î¸â‚‚, ..., âˆ‚J/âˆ‚Î¸â‚™]

The challenge: Your simulation is complex (Richards equation PDE!)
  - Takes minutes to run
  - Many parameters (you have 15!)
  - Gradient is expensive to compute


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 2: FINITE DIFFERENCES (THE SLOW WAY)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Finite differences approximate the gradient numerically:

    âˆ‚J/âˆ‚Î¸áµ¢ â‰ˆ [J(Î¸ + Îµeáµ¢) - J(Î¸)] / Îµ

where eáµ¢ is the i-th unit vector and Îµ is a small perturbation (e.g., 0.001).

COST FOR n PARAMETERS:
  - Baseline: 1 simulation to compute J(Î¸)
  - Gradients: n simulations for âˆ‚J/âˆ‚Î¸â‚, âˆ‚J/âˆ‚Î¸â‚‚, ..., âˆ‚J/âˆ‚Î¸â‚™
  - TOTAL: n + 1 simulations per iteration

YOUR CASE (15 parameters):
  - 16 simulations per iteration
  - 50 iterations
  - 800 total simulations
  - If 1 simulation = 60 seconds â†’ 13.3 HOURS! â°

PROBLEMS:
  âœ— Very expensive for many parameters
  âœ— Approximate gradients (numerical errors)
  âœ— Requires tuning epsilon (too small â†’ noise, too large â†’ bias)
  âœ— Doesn't scale well (doubles cost when you add a parameter)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 3: THE ADJOINT METHOD (THE FAST WAY)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The adjoint method computes EXACT gradients using automatic differentiation.

KEY INSIGHT: Instead of perturbing each parameter separately, we:
  1. Run the simulation ONCE forward
  2. Run ONE backward solve to get ALL gradients at once
  3. Use the chain rule automatically

COST FOR n PARAMETERS:
  - Forward: 1 simulation
  - Backward (adjoint): 1 solve (similar cost to forward)
  - TOTAL: ~2 simulations per iteration

YOUR CASE (15 parameters):
  - 2 solves per iteration (vs. 16 for finite differences!)
  - 50 iterations
  - 100 total solves
  - If 1 simulation = 60 seconds â†’ 1.7 HOURS! ğŸš€
  
SPEEDUP: 13.3 / 1.7 = 7.8Ã— FASTER!

ADVANTAGES:
  âœ“ Exact gradients (no numerical approximation errors)
  âœ“ Cost independent of number of parameters!
  âœ“ No epsilon tuning needed
  âœ“ More robust optimization
  âœ“ Scales to hundreds of parameters


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 4: WHAT IS THE "TAPE"?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The TAPE is a recording of all operations during the forward simulation.

Think of it like a movie recording ğŸ¬:
  - Each operation (addition, multiplication, PDE solve, etc.) is a frame
  - The tape stores HOW each variable was computed
  - We can "replay" the tape backwards to compute derivatives

EXAMPLE: Simple calculation
  
  Forward computation:
    x = 2.0          â† Parameter (input)
    a = x * 3        â† Operation 1: multiply by 3
    b = a + 5        â† Operation 2: add 5
    c = bÂ²           â† Operation 3: square
    J = c            â† Loss (output)
  
  Tape records:
    Block 1: a = x * 3
    Block 2: b = a + 5
    Block 3: c = bÂ²
    Block 4: J = c
  
  Adjoint (backward) pass:
    âˆ‚J/âˆ‚c = 1        (start from output)
    âˆ‚J/âˆ‚b = âˆ‚J/âˆ‚c Ã— 2b = 2b      (chain rule, block 3)
    âˆ‚J/âˆ‚a = âˆ‚J/âˆ‚b Ã— 1 = 2b       (chain rule, block 2)
    âˆ‚J/âˆ‚x = âˆ‚J/âˆ‚a Ã— 3 = 6b       (chain rule, block 1)
  
  Result: âˆ‚J/âˆ‚x = 6b, computed by going backwards through tape!


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 5: WHAT ARE "BLOCKS"?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BLOCKS are the individual operations recorded on the tape.

In Firedrake, common blocks include:

1. ASSIGNMENT BLOCKS
   Example: u.assign(initial_condition)
   Records: Variable u was set to a value
   
2. SOLVE BLOCKS  
   Example: solve(F == 0, u, bcs)
   Records: PDE was solved for variable u
   This is the most important block type!
   
3. ASSEMBLE BLOCKS
   Example: J = assemble(inner(u, u) * dx)
   Records: Integration was performed
   
4. FUNCTION UPDATE BLOCKS
   Example: u_n.assign(u)
   Records: Time-stepping update

TYPICAL TAPE SIZE:
  - Simple problem: 10-100 blocks
  - Your Richards solver: 500-5000 blocks
    * Each timestep adds multiple blocks
    * 100 timesteps Ã— ~10 blocks/step = ~1000 blocks
  
WHY SO MANY BLOCKS?
  Your simulation does many operations per timestep:
    1. Assemble PDE matrix
    2. Apply boundary conditions  
    3. Solve linear system
    4. Update solution
    5. Update material properties
    6. Probe extraction
    7. ... repeat for each timestep

Each of these creates blocks on the tape!


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 6: HOW ADJOINT OPTIMIZATION WORKS (STEP BY STEP)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STEP 1: SETUP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Convert parameters to Firedrake Constants:

  # Before (finite differences)
  theta_r = 0.02  # Just a Python float
  
  # After (adjoint)
  theta_r = Constant(0.02, name="theta_r")  # Firedrake Constant
  control = Control(theta_r)                # Wrap in Control for optimization

Why? Firedrake's automatic differentiation only works with Firedrake objects!


STEP 2: FORWARD SIMULATION (RECORDING)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Run simulation with annotation enabled:

  continue_annotation()  # Start recording
  simulated = your_richards_solver(param_constants)
  
What happens internally:
  
  Time     Operation                           Tape Block
  â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  t=0      uâ‚€.assign(initial_condition)    â†’   Block 0: Assignment
  t=Î”t     assemble(F)                     â†’   Block 1: Assemble
  t=Î”t     solve(F == 0, uâ‚, bcs)          â†’   Block 2: Solve
  t=Î”t     extract_probes(uâ‚)              â†’   Block 3: Function eval
  t=2Î”t    assemble(F)                     â†’   Block 4: Assemble
  t=2Î”t    solve(F == 0, uâ‚‚, bcs)          â†’   Block 5: Solve
  ...      ...                                  ...
  t=T      compute_loss(sim, obs)          â†’   Block N: Final loss

Result: Tape contains N blocks (N could be 500-5000 for your problem)


STEP 3: CREATE REDUCED FUNCTIONAL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  J = compute_loss(simulated, observations)  # Scalar loss value
  reduced_functional = ReducedFunctional(J, controls)

This creates a "smart function" that:
  - Knows how to replay the tape with new parameter values
  - Can automatically compute gradients via adjoint solve


STEP 4: OPTIMIZATION LOOP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Optimizer (L-BFGS-B) asks:
  
  [Iteration 1]
  Optimizer: "What's the loss at Î¸ = [0.02, 0.14, 0.94, ...]?"
  
  ReducedFunctional:
    1. Replays tape with Î¸ parameters
    2. Returns J(Î¸) = 0.123456
  
  Optimizer: "What's the gradient âˆ‡J at this point?"
  
  ReducedFunctional:
    1. Runs ADJOINT SOLVE (backward through tape)
    2. Uses chain rule to compute âˆ‚J/âˆ‚Î¸ for ALL parameters
    3. Returns âˆ‡J = [-0.001, 0.003, -0.002, ...]
    
    This is the magic! Instead of 16 forward simulations (finite diff),
    we do 1 forward + 1 backward = 2 solves total.
  
  Optimizer: "Thanks! I'll update Î¸ based on the gradient."
    Î¸_new = Î¸_old - learning_rate Ã— âˆ‡J
  
  [Iteration 2]
  Repeat...


STEP 5: ADJOINT SOLVE (THE BACKWARD PASS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

The adjoint solve goes BACKWARDS through the tape:

  Forward tape (recorded):
    Block 0: uâ‚€ = initial_condition
    Block 1: Aâ‚ = assemble(Fâ‚)  
    Block 2: uâ‚ = solve(Aâ‚, rhsâ‚)
    Block 3: Aâ‚‚ = assemble(Fâ‚‚)
    Block 4: uâ‚‚ = solve(Aâ‚‚, rhsâ‚‚)
    ...
    Block N: J = loss_function(uâ‚™)
  
  Adjoint (backward):
    Start: âˆ‚J/âˆ‚J = 1
    
    Block N: âˆ‚J/âˆ‚uâ‚™ = ???       (use chain rule)
    Block N-1: âˆ‚J/âˆ‚u_{n-1} = âˆ‚J/âˆ‚uâ‚™ Ã— âˆ‚uâ‚™/âˆ‚u_{n-1}
    ...
    Block 2: âˆ‚J/âˆ‚uâ‚ = ...
    Block 1: âˆ‚J/âˆ‚Aâ‚ = ...
    Block 0: âˆ‚J/âˆ‚Î¸ = âˆ‚J/âˆ‚uâ‚€ Ã— âˆ‚uâ‚€/âˆ‚Î¸    (GRADIENT!)

At each block, pyadjoint:
  1. Retrieves stored information from forward pass
  2. Applies chain rule to propagate derivatives backward
  3. Accumulates gradients with respect to parameters

For PDE solves (the expensive part):
  - Forward: Solve AÂ·u = b for u
  - Adjoint: Solve Aáµ€Â·Î» = âˆ‚J/âˆ‚u for Î» (similar cost!)
  - Then: âˆ‚J/âˆ‚Î¸ = Î»áµ€Â·âˆ‚A/âˆ‚Î¸Â·u

The adjoint solve is roughly the same cost as the forward solve!


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 7: WHY YOUR ERROR OCCURRED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Common error: "Tape is empty" or "No blocks recorded"

CAUSE: The loss computation wasn't recorded on the tape!

In the original code, the loss was computed using pure NumPy:
  
  # This doesn't get recorded! âœ—
  residuals = (simulated - observed)  # NumPy arrays
  loss = np.sum(residuals ** 2)       # NumPy operation
  
  # Tape has no blocks connecting loss to parameters!
  # Adjoint solve fails because there's no path back

SOLUTION: Use Firedrake operations or ensure operations are on tape:

  # Option 1: Compute loss during simulation
  probe_loss = Constant(0.0)
  for timestep in range(n_steps):
      solve(...)
      probe_val = u(probe_location)
      obs_val = Constant(observation[timestep])
      probe_loss += (probe_val - obs_val)**2  # On tape!
  
  # Option 2: Post-process with annotation
  # (The approach used in updated optimizer.py)

The key is that pyadjoint must be able to "see" the operations to 
differentiate through them!


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 8: CHECKING YOUR TAPE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

After running simulation, always check:

  from pyadjoint import get_working_tape
  
  n_blocks = len(get_working_tape().get_blocks())
  print(f"Tape has {n_blocks} blocks")
  
INTERPRETING RESULTS:

  n_blocks = 0:
    âœ— Nothing recorded! Problem:
      - continue_annotation() not called?
      - Parameters not Firedrake Constants?
      - Using NumPy instead of Firedrake operations?
  
  n_blocks = 1-10:
    âš  Too few blocks. Might be:
      - Very simple simulation
      - OR missing operations
  
  n_blocks = 100-1000:
    âœ“ Normal for Richards solver with timesteps
  
  n_blocks > 5000:
    âš  Very large tape, might use lots of memory
      Consider checkpointing for very long simulations


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 9: PRACTICAL WORKFLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. CONVERT PARAMETERS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   initial_params = {'theta_r': 0.02, 'theta_s': 0.14, ...}
   controls, constants = create_parameter_controls(initial_params)
   
   # constants['theta_r'] is now a Firedrake Constant
   # controls['theta_r'] is a Control for optimization

2. RUN SIMULATION WITH ANNOTATION
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   continue_annotation()  # START RECORDING!
   simulated = your_richards_solver(constants)
   
   # Check tape
   n_blocks = len(get_working_tape().get_blocks())
   if n_blocks == 0:
       print("ERROR: Tape is empty!")
       return

3. SETUP OPTIMIZATION
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   optimizer = AdjointOptimizer(observations, bounds)
   optimizer.setup_optimization(simulated, controls)
   
   # This creates ReducedFunctional(J, controls)

4. OPTIMIZE
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   best_params = optimizer.optimize(
       method='L-BFGS-B',
       maxiter=50,
       verbose=True
   )
   
   # scipy's L-BFGS-B will:
   #   - Call reduced_functional(Î¸) to get loss
   #   - Call reduced_functional.derivative() to get gradient
   #   - Update parameters
   #   - Repeat until convergence


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 10: MEMORY CONSIDERATIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The tape stores information from the forward solve, which uses memory.

TYPICAL MEMORY USAGE:
  - Small problem (10 timesteps): ~100 MB
  - Your problem (360 timesteps): ~500 MB - 2 GB
  - Large problem (1000+ timesteps): 5-20 GB

IF MEMORY IS AN ISSUE:
  
  1. CHECKPOINTING
     Store only every N-th state, recompute others during adjoint
     
     from firedrake.adjoint import set_working_tape
     from firedrake.adjoint.checkpointing import MultistageCheckpointSchedule
     
     schedule = MultistageCheckpointSchedule(n_steps, n_checkpoints=10)
     # Stores only 10 checkpoints instead of all 360 states
  
  2. SHORTER OPTIMIZATION PERIOD
     Optimize on subset of data (e.g., 1 month instead of 3 months)
  
  3. COARSER MESH
     Use 40Ã—20 mesh for optimization, then validate on 80Ã—40


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 11: TROUBLESHOOTING GUIDE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLEM: Tape is empty (0 blocks)
SOLUTION:
  1. Verify continue_annotation() called before simulation
  2. Check parameters are Firedrake Constants, not floats
  3. Ensure simulation uses Firedrake operations

PROBLEM: "Control not found" error
SOLUTION:
  Parameter must be wrapped in Control(Constant(...))
  Use create_parameter_controls() helper function

PROBLEM: Optimization doesn't improve loss
SOLUTION:
  1. Check bounds aren't too tight
  2. Try different optimizer (L-BFGS-B, SLSQP, TNC)
  3. Verify simulation is sensitive to parameters
  4. Check observation data quality

PROBLEM: First iteration very slow (5-10 minutes)
SOLUTION:
  This is NORMAL! Firedrake is:
  - Compiling the adjoint solve
  - Building derivative operations
  - Subsequent iterations will be much faster (2-3 minutes)

PROBLEM: "Singular matrix" or convergence failure
SOLUTION:
  1. Check PDE solver tolerances
  2. Verify boundary conditions are well-posed
  3. Try smaller timesteps
  4. Check parameter bounds (avoid extreme values)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART 12: SUMMARY - KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. THE TAPE
   âœ“ Recording of all operations during forward solve
   âœ“ Each operation is a "block" (assignment, solve, assemble, etc.)
   âœ“ Enables automatic differentiation via chain rule

2. BLOCKS
   âœ“ Individual operations on the tape
   âœ“ Your problem: 500-5000 blocks (one per operation per timestep)
   âœ“ Check: len(get_working_tape().get_blocks()) > 0

3. ADJOINT METHOD
   âœ“ Backward pass through tape to compute gradients
   âœ“ Cost: ~2Ã— forward solve (vs. (n+1)Ã— for finite differences)
   âœ“ Exact gradients, no epsilon tuning needed

4. SPEEDUP
   âœ“ 15 parameters: 7.8Ã— faster than finite differences
   âœ“ 50 parameters: 25Ã— faster!
   âœ“ Scales independently of parameter count

5. WORKFLOW
   âœ“ Parameters â†’ Firedrake Constants â†’ Controls
   âœ“ continue_annotation() â†’ run simulation â†’ check tape
   âœ“ setup_optimization() â†’ optimize() â†’ done!

6. YOUR CASE
   âœ“ 15 parameters (soil properties + rain + boundary conditions)
   âœ“ 3 observation probes
   âœ“ 360 timesteps (3 months)
   âœ“ Expected speedup: 7-8Ã— faster than finite differences
   âœ“ Runtime: ~2 hours for 50 iterations (vs. 13 hours)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REFERENCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Farrell et al. (2013): "Automated derivation of the adjoint of high-level 
                        transient finite element programs"
                        SIAM Journal on Scientific Computing

Firedrake Adjoint Docs: https://www.firedrakeproject.org/adjoint/
pyadjoint Documentation: http://www.dolfin-adjoint.org/

Good luck with your optimization! ğŸš€